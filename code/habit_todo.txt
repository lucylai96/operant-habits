habit todo

** todo **
model (priority)
- can make blahut arimoto reward complexity curves by counting state visitations and actions in those state (but this might be the opposite of what we want bc for ratio schedules, we basically do same action regardless of state and for interval schedules we do diff action in diff states) 
- does beta match with d(complex)/dV?
- empirical question: if beta keeps decreasing, does the cost increase?
- show that beta should increase for high contingency schedules (VR/FR > VI/FI)

- write a loop to go through models w diff params (diff schedules)
- show sensitivity to outcome devaluation for less vs more training 
- show less responding in VI for yolked 

(1) we can explain all these traditional findings (and which ones require the policy search, which don't?)
(2) can we show that beta is 
(3) can we show this matches better with data? 
(4) can we make new predictions?


** analysis **


questions for sam:
- is trial by trial cost calculated on the current state and action only?

** notes **
- "learned" O matrix is same for all schedules, only see reward if they have just pressed and end up in state 1 
- initialize O matrix to 0.5 for all?

beta
    - beta large: low cost, beta small: high cost
    - if beta gets smaller, it means that cost to pay is getting larger (VI/FI)
    - if beta gets larger, it means that cost to pay is getting smaller (VR/FR)

- we want beta to increase when mi<sched.cmax OR 0 <sched.cmax-mi
- if cost is less than cmax, you're allowed to increase policy cost a bit, so beta should increase!
- if cost less than cmax, beta should increase!
- if reward is negative, AND cost is not at max, beta decreases
- if reward positive and cost is not at max, beta increases

** questions for Eric Garr **
- devaluation  error bars are from pooling over animals? 
- how long is devaluation usually carried out before quantifying the press rate?